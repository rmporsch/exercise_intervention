{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b1ef82-5374-4209-8348-cd0179fde46f",
   "metadata": {},
   "source": [
    "# Model Training Notebook\n",
    "\n",
    "This notebooks goes through the process of model training and performance evaluation.\n",
    "\n",
    "## Used Framework and Modeling Choices\n",
    "\n",
    "Despite the given low amount of data I have opted for a deep learning approach using [Dart](https://unit8co.github.io/darts/index.html).\n",
    "Darts offers a number of pre-build deep learning approaches for time series data.\n",
    "However, in order to meet my project goals I have formed a number of minimun requirements. These include:\n",
    "- the model supports probabilistic outputs for a probablistic estimation if a given individual might change her behavior\n",
    "- multivariate input in order to make use of correlations within the data within and across individuals\n",
    "- support for covariates as behavior is often affected by external factors such as weekends, temperature and others\n",
    "\n",
    "Based on these requirements I used an implementation of a [Temporal Convolutional Network (TCN)](https://arxiv.org/abs/1906.04397).\n",
    "This model was preferred over the more classical RNN since it has shown better performance in past competitions. \n",
    "Further, it allowed me to try out this new model and to get some experience with its architecture and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "339607b5-2f03-43bc-a240-7eac09e0c831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.metrics import mape, smape\n",
    "from darts.models import ExponentialSmoothing, NaiveSeasonal, Prophet, TCNModel, NBEATSModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "\n",
    "from aws_lambda.chalicelib.data_download import S3, Settings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e8964-4934-4a80-b176-df743de2537c",
   "metadata": {},
   "source": [
    "# Data Access and preprocessing\n",
    "\n",
    "The original data was stored on an AWS S3 bucket to allow access through microservices and continious learning.\n",
    "Future version should consider a more stable data location, such a dedicated database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b3d370d3-4102-4c40-aa24-0322b68afb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from S3\n",
    "TMP_DIR = \"/tmp/training_data\"\n",
    "if not os.path.isdir(TMP_DIR):\n",
    "    os.mkdir(TMP_DIR)\n",
    "\n",
    "s3 = S3(Settings())\n",
    "s3.download(TMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b9549483-2662-4a89-8dee-24bbca475d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(TMP_DIR)\n",
    "# this file is ignored since it is not used in the project\n",
    "csv_files = {k.replace(\"_summary.csv\", \"\"): pd.read_csv(os.path.join(TMP_DIR, k)) for k in files if k != \"weightLog-report.csv\"}  \n",
    "# standardization of data\n",
    "for key, df in csv_files.items():\n",
    "    columns = df.columns\n",
    "    time_column = next(k for k in columns if k.endswith(\"Hour\") or k.endswith(\"Date\") or k == \"datetime\")\n",
    "    df.rename(columns={time_column: \"datetime\"}, inplace=True)\n",
    "    time_column = \"datetime\"\n",
    "    df[time_column] = pd.to_datetime(df[time_column], infer_datetime_format=True)\n",
    "    df[time_column] = df[time_column].values.astype(np.datetime64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3bba0-924e-4b0f-b385-3ac30a1a1c5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "The dataset has two distinct types of data: Daily and hourly records.\n",
    "In this project I will treat them as two distinct dataset and will not attempt to model the whole dataset together.\n",
    "However, with some more time a more holistic modeling approach might result in better results.\n",
    "\n",
    "### Hourly Records\n",
    "\n",
    "Daily records were modeling together across all subjects.\n",
    "This was done since daily records were available for similar variables, that is all variables related to exercises.\n",
    "While there might be no direct correlation between different subjects beyond the influence of common covariates such as time-of-the-day and day-of-the-week, the model might be able to make use of common patters across subjects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fe3ac981-014d-474a-9f08-c8d441d0b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_hourly(dt: pd.DataFrame, scaler: Scaler, variables: List[str], min_req_length: int = 7*24, fill_missing: bool = False):\n",
    "    idds_timeseries = []\n",
    "    used_idds = []\n",
    "    for idd, dd in dt.groupby(\"Id\"):\n",
    "        n, _ = dd.shape\n",
    "        if n < min_req_length:\n",
    "            continue\n",
    "        if not dd['datetime'].is_unique:\n",
    "            continue\n",
    "        if fill_missing:\n",
    "            _tts = TimeSeries.from_dataframe(dd,\"datetime\", variables, fill_missing_dates=True, freq=\"D\")\n",
    "        else:\n",
    "            _tts = TimeSeries.from_dataframe(dd,\"datetime\", variables)\n",
    "        used_idds.append(idd)\n",
    "        # convert to 32 for faster training\n",
    "        xdata = _tts.data_array().astype(np.float32)\n",
    "        _tts = TimeSeries.from_xarray(xdata)\n",
    "        idds_timeseries.append(_tts)\n",
    "    print(f\"Using {len(idds_timeseries)=}\")\n",
    "    scaled_idds_timeseries = [scaler.fit_transform(k) for k in idds_timeseries]\n",
    "    return scaled_idds_timeseries, used_idds\n",
    "\n",
    "def generate_covariates(time_series: List[TimeSeries], attributes: List[str], scalar: Scaler):\n",
    "    scaler_covariates = scalar\n",
    "    st_covariates = []\n",
    "    for ts in time_series:\n",
    "        _cov = []\n",
    "        for att in attributes:\n",
    "            _cov.append(datetime_attribute_timeseries(ts, attribute=att, one_hot=True))\n",
    "        # stacking\n",
    "        covariates = _cov.pop(0)\n",
    "        for c in _cov:\n",
    "            covariates = covariates.stack(c)\n",
    "        _scaled = scaler_covariates.fit_transform(covariates)\n",
    "        # convert to 32 for faster training\n",
    "        xdata = _scaled.data_array().astype(np.float32)\n",
    "        _scaled = TimeSeries.from_xarray(xdata)\n",
    "        st_covariates.append(_scaled)\n",
    "    return st_covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "32e00cff-777f-4a6b-99b3-e95a41014116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using len(idds_timeseries)=32\n"
     ]
    }
   ],
   "source": [
    "value_scaler = Scaler()\n",
    "variables = [\"TotalIntensity\", \"TotalSteps\", \"Calories\"]\n",
    "scaled_idds, used_idds = preprocessing_hourly(csv_files[\"hourlyActivity\"], value_scaler, variables, 7*24)\n",
    "covar_scaler = Scaler()\n",
    "st_covariates = generate_covariates(scaled_idds, ['hour', 'weekday'], covar_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "40e17eb3-a4e1-46c1-aaae-5028647d2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the last 72h of each series as test data\n",
    "train_val = [(k[:-72], k[-72:]) for k in scaled_idds]\n",
    "cov_train_val = [(k[:-72], k[-72:]) for k in st_covariates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c8e0dad0-303b-4df5-9b4a-27df84649394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.utils.likelihood_models import GaussianLikelihood \n",
    "model = TCNModel(\n",
    "    input_chunk_length=72,\n",
    "    output_chunk_length=6,\n",
    "    n_epochs=20,\n",
    "    random_state=0,\n",
    "    torch_device_str=\"cuda:0\",\n",
    "    log_tensorboard=True,\n",
    "    likelihood=GaussianLikelihood(),\n",
    "    model_name=\"daily_model\", \n",
    "    force_reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7bf4936a-d5b1-4dd4-bd6c-b95a21cdac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains, vals = zip(*train_val)\n",
    "cov_trains, cov_vals = zip(*cov_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ec197c6b-2792-4763-8741-bfa13a19c6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6667a5f5b16143ca9c8da12a01d01afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -183.3364\r"
     ]
    }
   ],
   "source": [
    "model.fit(trains, past_covariates=cov_trains, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc5049-f972-451f-b8a6-3a798684560c",
   "metadata": {},
   "source": [
    "### Predict ahead\n",
    "\n",
    "Next I will predict future values and store the relative output back into the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d0286500-aa56-4ce8-8292-4b0df195676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_future_covarites(dt: pd.DataFrame, freq: str, num: int, attributes: List[str], scaler: Scaler):\n",
    "    \"\"\"Generating covariates for future predictions\"\"\"\n",
    "    idx = pd.date_range(dt['datetime'].min(), dt['datetime'].max() + pd.Timedelta(days=20), freq=freq)\n",
    "    ts = pd.Series(range(len(idx)), index=idx)\n",
    "    ts = TimeSeries.from_series(ts)\n",
    "    tts = ts.add_datetime_attribute(\"weekday\")\n",
    "    tts = tts.add_datetime_attribute(\"month\")\n",
    "    \n",
    "    covariates_all = generate_covariates([tts], attributes, scaler)\n",
    "    _scaled = scaler.fit_transform(covariates_all)\n",
    "    xdata = _scaled[0].data_array().astype(np.float32)\n",
    "    _scaled = TimeSeries.from_xarray(xdata)\n",
    "    covariates_all = [covariates_all[0] for _ in range(num)]\n",
    "    return covariates_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4c947dd1-a731-4697-8853-ef563228c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_covariates =  generate_future_covarites(csv_files[\"hourlyActivity\"],\"H\", len(trains), ['hour', 'weekday'], covar_scaler)\n",
    "prediction = model.predict(12, vals, past_covariates=all_covariates, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6b74f336-aeb6-4219-bd07-c00436d19564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(prediction, output_name, used_idds, scaler, input_dt):\n",
    "    # get prediction quantiles\n",
    "    quantiles = [0.1, 0.5, 0.90]\n",
    "    new_q_names = [\"_lower\", \"_mean\", \"_upper\"]\n",
    "    qq_names = {v: k for k, v in zip(new_q_names, quantiles)}\n",
    "\n",
    "    pred_q = {}\n",
    "    for q in quantiles:\n",
    "        pred_q[q] = [scaler.inverse_transform(k.quantile_timeseries(q)) for k in prediction]\n",
    "        \n",
    "    # convert to df\n",
    "    predicted_data = {}\n",
    "    for idd in range(len(used_idds)):\n",
    "        pred_qq_idd = []\n",
    "        for qq, data in pred_q.items():\n",
    "            _d = data[idd].pd_dataframe().applymap(lambda x: float(x))\n",
    "            cc = _d.columns\n",
    "            new_names = [u.replace(f\"_{qq}\", qq_names[qq]) for u in cc]\n",
    "            _d.rename(columns={k: v for k, v in zip(cc, new_names)}, inplace=True)\n",
    "            pred_qq_idd.append(_d)\n",
    "        predicted_data[used_idds[idd]] = pred_qq_idd\n",
    "        \n",
    "    # converting the data to the required fromat for the microservice\n",
    "    idds_predicted_data = []\n",
    "    for idd, data_list in predicted_data.items():\n",
    "        _new_data = pd.concat(data_list, axis=1)\n",
    "        _new_data['Id'] = str(idd)\n",
    "        _new_data['datetime'] = _new_data.index\n",
    "        _new_data['has_prediction'] = 1\n",
    "        _new_data['has_values'] = 0\n",
    "        idds_predicted_data.append(_new_data)\n",
    "\n",
    "    combined_prediction = pd.concat(idds_predicted_data, axis=0, ignore_index=True)\n",
    "    input_dt['has_prediction'] = 0\n",
    "    input_dt['has_values'] = 1\n",
    "\n",
    "    combined_dt = pd.concat([input_dt, combined_prediction], axis=0, ignore_index=True)\n",
    "    combined_dt.to_csv(output_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "209d1b96-d6a7-4dd6-acdd-147ecea11987",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction(prediction, \"prediction_test/hourlyActivity_summary.csv\", used_idds, value_scaler, csv_files['hourlyActivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "43d4cc39-ee37-4f73-8f7a-394880476b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtesting(model, series: List[TimeSeries], covar: List[TimeSeries]):\n",
    "    output = []\n",
    "    for s, c in zip(series, covar):\n",
    "        backtest_cov = model.historical_forecasts(s, past_covariates=c, start=0.5, forecast_horizon=1, stride=1, retrain=False, verbose=False)\n",
    "        output.append(smape(s, backtest_cov))\n",
    "    return np.mean(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "532f59f3-a129-494f-9549-0cd1db050655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAPE (using covariates) = 70.95%\n"
     ]
    }
   ],
   "source": [
    "mean_smape = backtesting(model, trains, cov_trains)\n",
    "print('SMAPE (using covariates) = {:.2f}%'.format(average_smape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0d243-6bd1-470b-98a2-c7fb04b6d18e",
   "metadata": {},
   "source": [
    "# Daily Records\n",
    "\n",
    "In contrast daily records are rather diverse and cover a variety of different behaviors. This includes sleep, weight as well as activities.\n",
    "Unfortunately, the amount of subjects who have data across all these areas are very limited. Preventing a complete modeling approach across all variables and subjects.\n",
    "Hence, I have opted to model each area separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98deb4d-9d9a-454c-a61f-9785eed1e696",
   "metadata": {},
   "source": [
    "## Daily Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c5de2704-3664-47be-b508-138d140ef516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using len(idds_timeseries)=29\n"
     ]
    }
   ],
   "source": [
    "value_scalar = Scaler()\n",
    "covar_scalar = Scaler()\n",
    "ddf_name = 'dailyActivity'\n",
    "dddf = csv_files[ddf_name]\n",
    "\n",
    "variables = [k for k in dddf.columns if not any(u in k for u in (\"Id\", \"datetime\"))]\n",
    "ts_series, idds_selected = preprocessing_hourly(dddf, value_scalar, variables, min_req_length=7, fill_missing=True)\n",
    "ts_series = [fill_missing_values(k) for k in ts_series]  #fill missing values here, while this is not ideal for a production model for this POC it is sufficient\n",
    "ts_covar = generate_covariates(ts_series, ['weekday', 'month'], covar_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d5d842b7-c8a4-42b2-97ee-cc27eacaa660",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_model_activity = TCNModel(\n",
    "    input_chunk_length=7,\n",
    "    output_chunk_length=2,\n",
    "    n_epochs=20,\n",
    "    random_state=0,\n",
    "    torch_device_str=\"cuda:0\",\n",
    "    # log_tensorboard=True,\n",
    "    likelihood=GaussianLikelihood(),\n",
    "    model_name=f\"daily_model_{ddf_name}\",\n",
    "    batch_size=2,\n",
    "    force_reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c7288efb-c440-4b51-a30a-ea35cfac3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the last 7h of each series as test data\n",
    "train_val = [(k[:-7], k[-7:]) for k in ts_series]\n",
    "cov_train_val = [(k[:-7], k[-7:]) for k in ts_covar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d668317b-2d8f-4cf6-ae8c-33b561391aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains, vals = zip(*train_val)\n",
    "cov_trains, cov_vals = zip(*cov_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a1b2a886-3f59-42d1-a7c2-1275202356c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fcc62a87e74af9b94d28aec6e0e320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -33.8674\r"
     ]
    }
   ],
   "source": [
    "daily_model_activity.fit(trains, past_covariates=cov_trains, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2f325954-2b02-4a6e-8bf0-4d8f7840cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_covariates =  generate_future_covarites(dddf, \"D\", len(trains), ['month', 'weekday'], covar_scaler)\n",
    "prediction = daily_model_activity.predict(2, vals, past_covariates=all_covariates, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5ef4af13-125e-4c82-9561-e0fb7f9016f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction(prediction, \"prediction_test/dailyActivity_summary.csv\", idds_selected, value_scalar, dddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7ea4b221-b7ad-4856-b8d0-b82ec794c3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAPE (using covariates) = 70.95%\n"
     ]
    }
   ],
   "source": [
    "mean_smape = backtesting(daily_model_activity, ts_series, ts_covar)\n",
    "print('SMAPE (using covariates) = {:.2f}%'.format(average_smape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536cffe-0ed3-41d4-8f80-4727f9649985",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Daily Sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d68728ee-74ef-4307-a4eb-79cdd9f45af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using len(idds_timeseries)=13\n"
     ]
    }
   ],
   "source": [
    "value_scalar = Scaler()\n",
    "covar_scalar = Scaler()\n",
    "ddf_name = 'dailySleep'\n",
    "dddf = csv_files[ddf_name]\n",
    "\n",
    "variables = [k for k in dddf.columns if not any(u in k for u in (\"Id\", \"datetime\"))]\n",
    "ts_series, idds_selected = preprocessing_hourly(dddf, value_scalar, variables, min_req_length=7, fill_missing=True)\n",
    "ts_series = [fill_missing_values(k) for k in ts_series]  #fill missing values here, while this is not ideal for a production model for this POC it is sufficient\n",
    "ts_covar = generate_covariates(ts_series, ['weekday', 'month'], covar_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "62f59f3d-96e2-4f81-bc21-5896d9e1319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_model_sleep = TCNModel(\n",
    "    input_chunk_length=7,\n",
    "    output_chunk_length=2,\n",
    "    n_epochs=20,\n",
    "    random_state=0,\n",
    "    torch_device_str=\"cuda:0\",\n",
    "    # log_tensorboard=True,\n",
    "    likelihood=GaussianLikelihood(),\n",
    "    model_name=f\"daily_model_{ddf_name}\",\n",
    "    batch_size=2,\n",
    "    force_reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7e0fdd58-8f7d-46a0-8f2a-71175c40ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the last 7h of each series as test data\n",
    "train_val = [(k[:-7], k[-7:]) for k in ts_series]\n",
    "cov_train_val = [(k[:-7], k[-7:]) for k in ts_covar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "73c0b455-6321-48c7-b255-ad160cf92c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains, vals = zip(*train_val)\n",
    "cov_trains, cov_vals = zip(*cov_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "02da1579-831c-464a-8fbc-6a6f988583c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a6d0375ddf4afab53d7b19c6c882d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -60.3815\r"
     ]
    }
   ],
   "source": [
    "daily_model_sleep.fit(trains, past_covariates=cov_trains, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "423ed7ee-ffc1-4db2-a375-05326a18f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_covariates =  generate_future_covarites(dddf, \"D\", len(trains), ['month', 'weekday'], covar_scaler)\n",
    "prediction = daily_model.predict(2, vals, past_covariates=all_covariates, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "27ed7cd6-93f1-4e44-bc79-844eb88e4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction(prediction, \"prediction_test/dailySleep_summary.csv\", idds_selected, value_scalar, dddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "657115f2-7432-4b9c-9828-2982ee4b6594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAPE (using covariates) = 70.95%\n"
     ]
    }
   ],
   "source": [
    "mean_smape = backtesting(daily_model_sleep, ts_series, ts_covar)\n",
    "print('SMAPE (using covariates) = {:.2f}%'.format(average_smape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54116ff-4dd7-491c-bc16-5c0f00de5d68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Daily Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6d90ff50-7f2c-4f71-af04-0ff286527819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using len(idds_timeseries)=2\n"
     ]
    }
   ],
   "source": [
    "value_scalar = Scaler()\n",
    "covar_scalar = Scaler()\n",
    "ddf_name = 'dailyWeightLog'\n",
    "dddf = csv_files[ddf_name]\n",
    "\n",
    "variables = [k for k in dddf.columns if not any(u in k for u in (\"Id\", \"datetime\"))]\n",
    "ts_series, idds_selected = preprocessing_hourly(dddf, value_scalar, variables, min_req_length=7, fill_missing=True)\n",
    "ts_series = [fill_missing_values(k) for k in ts_series]  #fill missing values here, while this is not ideal for a production model for this POC it is sufficient\n",
    "ts_covar = generate_covariates(ts_series, ['weekday', 'month'], covar_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d535e48e-6798-4015-bdf6-5e3926ab0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_model_sleep = TCNModel(\n",
    "    input_chunk_length=7,\n",
    "    output_chunk_length=2,\n",
    "    n_epochs=20,\n",
    "    random_state=0,\n",
    "    torch_device_str=\"cuda:0\",\n",
    "    # log_tensorboard=True,\n",
    "    likelihood=GaussianLikelihood(),\n",
    "    model_name=f\"daily_model_{ddf_name}\",\n",
    "    batch_size=2,\n",
    "    force_reset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c1e16859-261f-4016-9575-fb5acc63d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the last 7d of each series as test data\n",
    "train_val = [(k[:-7], k[-7:]) for k in ts_series]\n",
    "cov_train_val = [(k[:-7], k[-7:]) for k in ts_covar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ff39f0ca-037b-44a1-9c5e-e8b80708a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains, vals = zip(*train_val)\n",
    "cov_trains, cov_vals = zip(*cov_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c0352416-6cf5-45d9-b184-776a7e8303fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80255aa01daa40a186c7b096abeae109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 41.37551\r"
     ]
    }
   ],
   "source": [
    "daily_model_sleep.fit(trains, past_covariates=cov_trains, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c1ff5044-1cc2-4558-b4f8-8774cee93eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_covariates =  generate_future_covarites(dddf, \"D\", len(trains), ['month', 'weekday'], covar_scaler)\n",
    "prediction = daily_model_sleep.predict(2, vals, past_covariates=all_covariates, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "f7b11f30-bff8-41fc-a55e-810bc70d7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction(prediction, \"prediction_test/dailyWeightLog_summary.csv\", idds_selected, value_scalar, dddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ba532feb-0294-4fde-9b86-b92e4d85434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAPE (using covariates) = 94.92%\n"
     ]
    }
   ],
   "source": [
    "average_smape = backtesting(daily_model_sleep, ts_series, ts_covar)\n",
    "print('SMAPE (using covariates) = {:.2f}%'.format(average_smape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce70984-5144-49b0-bbcf-85725da1ce15",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17504fae-b726-4870-8a74-101fa159f48b",
   "metadata": {},
   "source": [
    "Due to the limited training data a comprehensive model performance analysis is rather difficult.\n",
    "As indicated by the sMAPE score, current performance is not desirable. However, given the limited sample size as well as the large data requirement of deep learning model these results are not very surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d4848-2294-40b3-b0ac-b8950d8f3038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
